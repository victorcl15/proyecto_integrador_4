# Proyecto: Pipeline de Datos de E-Commerce

Para resolver este proyecto y crear el pipeline ELT, deberÃ¡s completar los archivos de Python (.py) y SQL (.sql) que estÃ¡n distribuidos en diferentes carpetas. A continuaciÃ³n, te proporcionamos las instrucciones sobre cÃ³mo proceder y el orden de ejecuciÃ³n:

## 1. ExtracciÃ³n

En la fase de extracciÃ³n de datos del pipeline, tendrÃ¡s que completar todas las funciones que tengan la marca **TODO** dentro del mÃ³dulo `src/extract.py`.

Si deseas verificar que tu cÃ³digo cumple con los requisitos, puedes probar ese mÃ³dulo en particular ejecutando el siguiente comando:

```console
$ pytest tests/test_extract.py
```

## 2. Carga

Ahora que tienes todos los datos de diferentes fuentes, es momento de almacenarlos en un Data Warehouse. Usaremos SQLite como nuestro motor de base de datos para mantener la simplicidad, aunque en empresas mÃ¡s grandes, Snowflake es una de las opciones mÃ¡s populares para Data Warehouses.

Por favor, completa todas las funciones con la marca **TODO** dentro del mÃ³dulo `src/load.py`.

## 3. TransformaciÃ³n

Teniendo los datos almacenados en el Data Warehouse, puedes empezar a hacer consultas y transformaciones.

Para esta tarea, ya te proporcionamos el cÃ³digo necesario dentro del mÃ³dulo `src/transform.py`, pero tendrÃ¡s que escribir las consultas SQL ğŸ˜¬.

Por favor, completa todos los scripts `.sql` con la marca **TODO** dentro de la carpeta `queries/`.

Puedes usar herramientas como DBeaver para escribir y probar las consultas de forma interactiva. Finalmente, puedes verificar que tus consultas cumplen con los requisitos ejecutando las pruebas proporcionadas con el siguiente comando:

```console
$ pytest tests/test_transform.py
```

AdemÃ¡s, puedes validar cÃ³mo debe verse el resultado de la consulta revisando el archivo `.json` en `tests/query_results` que tiene el mismo nombre que el archivo `.sql` correspondiente.

## 4. Visualiza tus resultados

Finalmente, una vez que tengas todos los resultados de tus consultas, es hora de crear algunas visualizaciones para la presentaciÃ³n.

Para esto, crea un dashboard que de respuesta al problema de negocio:
*EstÃ¡s trabajando para uno de los sitios de comercio electrÃ³nico mÃ¡s grandes de LatinoamÃ©rica, y el equipo de Ciencia de Datos ha recibido la solicitud de analizar datos de la compaÃ±Ã­a para comprender mejor su desempeÃ±o en ciertas mÃ©tricas durante los aÃ±os 2016-2018.*
*Hay dos Ã¡reas principales que desean explorar: Ingresos y Entregas.*
*BÃ¡sicamente, quieren entender cuÃ¡nto ingresaron por aÃ±o, cuÃ¡les fueron las categorÃ­as de productos mÃ¡s y menos populares, y los ingresos por estado. Por otro lado, tambiÃ©n es importante conocer quÃ© tan bien estÃ¡ entregando la compaÃ±Ã­a los productos vendidos en tiempo y forma a sus usuarios. Por ejemplo, ver cuÃ¡nto tiempo toma entregar un paquete dependiendo del mes, y la diferencia entre la fecha estimada de entrega y la fecha real.*

Piensa tambiÃ©n en preguntas adicionales que puedas resolver con los datos y que den valor al problema de negocio. 

Recuerda que tu dashboard debe tener mÃ¡ximo 5 grÃ¡ficos.

## 5. OrquestaciÃ³n de Datos con Apache Airflow

En esta tarea, te pedimos que reutilices el cÃ³digo del pipeline ELT actual para crear un DAG de Airflow que automatice todo el proceso.

Por favor, no modifiques ni cambies la estructura del proyecto actual ni el cÃ³digo que pueda romper las pruebas unitarias proporcionadas. En su lugar, te sugerimos que trabajes en una nueva carpeta dentro del proyecto y coloques allÃ­ el cÃ³digo de los DAGs. Esta tarea es abierta, por lo que puede tener mÃ¡s de una respuesta, soluciÃ³n o resultado correcto, y puede completarse de varias maneras. Si tienes tiempo y quieres desafiarte, Â¡adelante!

## Estructura del Proyecto

Antes de empezar a trabajar, revisemos la estructura general del proyecto y cada uno de sus mÃ³dulos:

```console
â”œâ”€â”€ dataset
â”‚Â Â  â”œâ”€â”€ olist_customers_dataset.csv
â”‚Â Â  â”œâ”€â”€ olist_geolocation_dataset.csv
â”‚Â Â  â”œâ”€â”€ olist_order_items_dataset.csv
â”‚Â Â  â”œâ”€â”€ olist_order_payments_dataset.csv
â”‚Â Â  â”œâ”€â”€ olist_order_reviews_dataset.csv
â”‚Â Â  â”œâ”€â”€ olist_orders_dataset.csv
â”‚Â Â  â”œâ”€â”€ olist_products_dataset.csv
â”‚Â Â  â”œâ”€â”€ olist_sellers_dataset.csv
â”‚Â Â  â””â”€â”€ product_category_name_translation.csv
â”œâ”€â”€ images
â”œâ”€â”€ queries
â”œâ”€â”€ src
â”œâ”€â”€ tests
â”œâ”€â”€ instrucciones.md
â”œâ”€â”€ Project.ipynb
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

### dataset
Contiene todos los archivos .csv con la informaciÃ³n que usarÃ¡s en el proyecto.

### queries
Contiene todas las consultas SQL que debes completar para luego generar tablas y grÃ¡ficos.

### src
Contiene diferentes archivos fuente necesarios para que todo el proyecto funcione.

### tests
Carpeta con los archivos necesarios para probar el proyecto.

### Otros
- `instrucciones.md`: InformaciÃ³n clave para entender el proyecto.
- `Project.ipynb`: Notebook que unifica las partes del proyecto y te indica los pasos a seguir.
- `README.md`: Archivo con la descripciÃ³n del proyecto.
- `requirements.txt`: Lista de librerÃ­as necesarias.

Con esto, ya puedes empezar. Â¡Buena suerte!

